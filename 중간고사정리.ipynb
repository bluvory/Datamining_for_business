{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "\n",
    "# 회귀분석\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 독립변수 선정\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy.stats.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "# 다중 공선성 문제\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# 로지스틱 회귀분석\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# 릿지, 라쏘\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# 규제\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# 의사결정나무\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dtreeplt import dtreeplt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 나이브 베이즈\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 서포트 벡터 머신\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# 앙상블\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# 랜덤 포레스트\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# 부스팅\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 확인\n",
    "- 회귀: RMSE\n",
    "- 분류: roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "mae = sum( abs(y_pred - y_test) / len(y_pred))\n",
    "rmse = math.sqrt( sum((y_pred - y_test)**2) / len(y_pred) )\n",
    "mape = ((abs(y_pred - y_test)/y_pred)*100).mean()\n",
    "rmse = mean_squared_error(y_pred, y_test)\n",
    "rmse = math.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print('MAE: ', mae)\n",
    "print('MAE: ', mape)\n",
    "print('MSE: ', mse)\n",
    "print('MAE: ', rmse)\n",
    "\n",
    "#######################################################\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "scores = cross_val_score(model, x, y, scoring='neg_mean_squared_error',cv=5)\n",
    "scores.mean()\n",
    "\n",
    "######################################################\n",
    "\n",
    "print(\"학습 집합 정확도: {:.3f}\".format(model.score(x_train, y_train)))\n",
    "print(\"테스트 집합 정확도: {:.3f}\".format(model.score(x_test, y_test)))\n",
    "\n",
    "####################################################### \n",
    "\n",
    "print(\"테스트 집합 AUC 점수: {:.3f}\".format(roc_auc_score(y_test, y_pred)))\n",
    "scores = cross_val_score(model, x, y, scoring='roc_auc',cv=5)\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2주차 회귀분석 (회귀!)\n",
    "지도 학습 방안 중 하나이며, 수치를 예측하는 것에 활용\n",
    "- 모든 데이터에 대해 오차를 계산하고 오차의 합을 최소로 만드는 직선을 찾기\n",
    "- 관찰된 연속형 변수들인 독립변수와 종속변수 사이의 상관관계에 따른 수학적 모델인 선형 관계식을 구하여 어떤 독립변수가 주어졌을 때 이에 따른 종속변수의 값을 예측함\n",
    "- 종속변수: 연속형인 수치 변수 / 독립변수: 연속형, 범주형인 수치 데이터\n",
    "- 독립변수 하나: 단순 회귀 분석 / 여러개: 다중 회귀 분석\n",
    "- 상관관계가 있다 != 인과관계가 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodels 이용\n",
    "model = sm.OLS(y, x).fit()\n",
    "pred = model.predict(x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정계수 R-squared\n",
    "- 종속변수의 총 변동에 대비해 회귀 모형이 얼마나 그 변동을 설명하는지 뜻함\n",
    "- 범위: 0~1 => 1에 가까울수록 회귀 모형이 종속변수의 변동 많이 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 회귀 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, x).fit()\n",
    "pred = model.predict(x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수정결정계수 Adjusted R-squared\n",
    "- 독립변수를 증가시켰음에도 불구하고 회귀모형의 설명력이 증가하지 않은 경우에는 오히려 값이 줄어듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 독립변수 선정\n",
    "- 0.1 ~ 0.3 : 약한 상관관계\n",
    "- 0.3 ~ 0.5 : 중간 상관관계\n",
    "- 0.5 ~ 1.0 : 강한 상관관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scatter_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5ef993d7e431>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# scatter matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscatter_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 피어슨 상관관계\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pearson'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scatter_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# scatter matrix\n",
    "scatter_matrix(data_train, figsize=(6,6))\n",
    "\n",
    "# 피어슨 상관관계\n",
    "data.corr(method='pearson')\n",
    "\n",
    "# 히트맵\n",
    "cor = x_train.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공선성, 다중 공선성\n",
    "- 공선성: 두 개 또는 그 이상의 독립변수들이 서로 밀접하게 상관되어 있는 경우\n",
    "- 다중공선성 문제: 어떤 독립변수 쌍도 특별히 높은 상관성을 가지지 않아도 세개 또는 그 이상의 변수들 사이에 공선성 존재\n",
    "- 판단: Variance Inflation Factor 1이 최저값이며 작을수록 좋지만 10이하면 문제없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vif(inputs):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = inputs.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(inputs.values, i) for i in range(inputs.shape[1])]\n",
    "    return(vif)\n",
    "\n",
    "calc_vif(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수 제거를 통한 독립변수 선정\n",
    "독립변수들의 조합에 따라 입력후보 변수를 선정하고 이때의 성과를 측정하여 가장 성과가 좋은 변수의 조합 찾기\n",
    "- 후진제거법\n",
    "- 1) 모든 독립변수를 가지고 모델을 학습한다.\n",
    "- 2) 독립변수 중에서 유의하지 않으며 가장 p-value 값이 높은 독립변수를 제거한다.\n",
    "- 3) 2)에서 제거된 독립변수외의 다른 독립변수를 가지고 모델을 학습한다.\n",
    "- 4) 2)-3)을 반복하며, 모든 독립변수가 유의하면 멈춘다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y,x).fit()\n",
    "model.pvalues\n",
    "\n",
    "x = x.drop(['columns'], axis=1)\n",
    "model = sm.OLS(y,x).fit()\n",
    "model.pvalues\n",
    "\n",
    "# 하나씩 제거해가면서 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import trian_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차검증\n",
    "- 데이터 집합을 체계적으로 바꿔나가면서 모든 데이터에 대해 모형의 성과를 측정하는 방식\n",
    "- 전체 데이터 집합을 학습 집합과 테스트 집합으로 나눠서 모형 학습에 학습 데이터 집합을 사용하고 학습된 모형의 성과를 평가하는 테스트 집합을 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "scores = cross_val_score(model, x, y, scoring='neg_mean_absolute_error',cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3주차 로지스틱 회귀분석 (분류!)\n",
    "1. 분류: 미리 정의된 여러 개의 범주(category or class) 중 하나에 할당\n",
    "2. 비용 함수: 기계학습 모형이 학습하면서 최소화해야 하는 함수\n",
    "3. 오차 행렬: 분류 문제의 성과를 측정하기 위한 기본적인 데이터\n",
    "4. 정확도: 실제 양성, 음성인 데이터를 예측에서도 양성, 음성으로 정확하게 분류한 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 분석이 범주를 분류하는 결정경계 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력변수 하나일때\n",
    "X_new = np.linspace(6,32,1000).reshape(-1,1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:,1], \"g-\", label=\"Yes\")\n",
    "plt.plot(X_new, y_proba[:,0], \"b--\", label=\"No\")\n",
    "\n",
    "\n",
    "# 입력변수 두개일때\n",
    "y_predict = log_reg.predict(X)\n",
    "X['predict'] = y_predict\n",
    "X_1 = X[X['predict']==1].copy()\n",
    "X_0 = X[X['predict']==0].copy()\n",
    "plt.plot(X_1['Frequency'], X_1['Recency'], 'bo')\n",
    "plt.plot(X_0['Frequency'], X_0['Recency'], 'ro')\n",
    "plt.xlabel('Purchase Frequency')\n",
    "plt.ylabel('Purchase Recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## smi로 회귀분석 ##\n",
    "ogit_model=sm.Logit(y,x)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다항 로지스틱 회귀\n",
    "- LogisticRegression: 이진 분류에 사용되지만\n",
    "- multi_class='multinomial': 다항 로지스틱 회귀에 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "softmax_reg.fit(X_train,y_train)\n",
    "\n",
    "y_prob = softmax_reg.predict_proba(X_test)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_p = lb.fit_transform(y_test)\n",
    "\n",
    "log_loss(y_p, y_prob)\n",
    "softmax_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 성과 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정밀도와 채현율의 상충관계 ##\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "y_score = log_reg.predict_proba(X_test)\n",
    "y_score[:,1] # 1일 확률\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_score[:,1])\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, threholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "    plt.vlines(x=0.5,ymin=0,ymax=1)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()\n",
    "               \n",
    "               \n",
    "## ROC 곡선 ##\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score[:,1])\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([[0],[1]], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.grid(which='major', axis='both', alpha=0.5)\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4주차 릿지, 라쏘, 엘라스틱넷 회귀 분석 (회귀!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 과적합(Overfitting): 학습한 모형이 학습 데이터에 과하게 적합되어 학습에 사용되지 않은 테스트 데이터는 정확하게 예측하거나 분류하지 못하는 경우\n",
    "2. 규제(Regularization): 모형을 단순하게 하고 과대적합의 위험을 감소시키기 위해 모형에 제약을 가하는 것\n",
    "3. 하이퍼파라미터(Hyper parameter): 모형에 따라 분석가가 지정해 주어야 하는 값이며, 최적의 값을 분석가가 찾아야하기에 튜닝 파라미터(tuning parameter)라고 불리기도 함\n",
    "4. 그리드 탐색: 적합한 하이퍼파라미터를 탐색하기 위해서 가능한 모든 하이퍼파라미터 조합에 대해 성과를 측정하여 최적의 하이퍼파라미터 조합을 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 규제 있는 회귀 모형들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 릿지 회귀 : L2 노름이 규제로 추가된 선형 회귀 모형 ##\n",
    "import sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=0)\n",
    "ridge_reg.fit(x,y)\n",
    "y_pred = ridge_reg.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라쏘 회귀 : L1 노름이 규제로 추가된 선형 회귀 모형 ##\n",
    "import sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0)\n",
    "lasso_reg.fit(x,y)\n",
    "y_pred = lasso_reg.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 엘라스틱넷 회귀 : 릿지회귀+라쏘회귀 (혼합비율 gamma 사용) ##\n",
    "import sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=1, l1_ratio=0.5)   # gamma=0:릿지 / gamma=1:라쏘\n",
    "elastic_net.fit(x,y)\n",
    "y_pred = elastic_net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 규제\n",
    "- 규제가 있는 회귀 모형은 변수의 범위에 따라 민감하게 반응하므로 범위 일치시켜주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler : 평균 0 표준편차 1 인 표준정규분포\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x))\n",
    "\n",
    "\n",
    "# MinMaxScaler : 최댓값,최솟값을 이용해 0과 1사이의 범위로 변환\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "x = pd.DataFrame(scaler.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 활용 ##\n",
    "\n",
    "# 기울기\n",
    "ridge_reg.coef_\n",
    "lasso_reg.coef_\n",
    "elastic_net.coef_\n",
    "\n",
    "# 확인\n",
    "from regressors import stats\n",
    "stats.summary(ridge_reg, x, y)\n",
    "stats.summary(lasso_reg, x, y)\n",
    "stats.summary(elastic_net, x, y)\n",
    "\n",
    "# 성능 확인\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "rmse_ridge = math.sqrt(mean_squared_error(y_ridge, y_test))\n",
    "rmse_lasso = math.sqrt(mean_squared_error(y_lasso, y_test))\n",
    "rmse_elastic = math.sqrt(mean_squared_error(y_elastic, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 튜닝\n",
    "- 최적의 하이퍼 파라미터 조합 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 그리드 탐색 ##\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'alpha':[0.1,0.5,1,1.5,2], 'l1_ratio':[0,0.25,0.5,0.75,1]}\n",
    "grid_search = GridSearchCV(elastic_net,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(x,y)\n",
    "grid_search.best_params_     # 최적의 파라미터\n",
    "grid_search.best_estimator_  # 최적의 파라미터 조합\n",
    "\n",
    "\n",
    "\n",
    "## 랜덤 탐색 ##\n",
    "from scipy.stats as stats\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 값으로 지정\n",
    "param_list = {'alpha':[0.1,0.5,1,1.5,2], 'l1_ratio':[0,0.25,0.5,0.75,1]}\n",
    "# 균등분포로 지정\n",
    "param_list = {'alpha':stats.uniform(0,1), 'l1_ratio':loguniform(1e-4, 1e0)}\n",
    "n_iter = 20     # 지정된 수만큼 이 중에서 무작위로 성과 측정\n",
    "rand_search = RandomizedSearchCV(model,\n",
    "                                 param_distributions = param_list,\n",
    "                                 cv = 5,\n",
    "                                 n_iter = n_iter,\n",
    "                                 scoring = 'neg_mean_squared_error',\n",
    "                                 return_train_score=True)\n",
    "rand_search.fit(x,y)\n",
    "rand_search.best_params_\n",
    "rand_search.best_estimator_\n",
    "\n",
    "\n",
    "# 조합마다 평가 (조합마다의 성과가 rmse로 출력)\n",
    "cvres = rand_serach.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score),params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 로지스틱 선형회귀에서의 하이퍼 파라미터 튜닝 : L2노름을 기본 규제 ##\n",
    "log_reg = LogisticRegression(solver='newton-cg')\n",
    "param_grid = [{'C':[1e-2,0.1,0.3,0.5,0.7,0.9,1,5,10,20]}]\n",
    "log_reg = LogisticRegression(C=grid_search.best_params_['C'].fit(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5주차 의사결정나무 (분류!)\n",
    "대표적인 분류 기법의 하나, 목표 변수의 범주를 기준으로 동일한 범주의 데이터들끼리 분류하는 규칙을 반복적으로 만들어나가는 방안\n",
    "\n",
    "1. 뿌리 노드(root node): 의사결정나무의 시작점으로 모든 데이터가 포함되는 노드이다.\n",
    "2. 말단 노드(leaf node): 의삭결정나무 분할 규칙에 의해서 더 이상 나누어 지지 않는 노드이며, 따라서 자식 노드를 갖지 않는다.\n",
    "3. 지니 불순도: 하나의 노드에 서로 다른 범주의 데이터가 섞여있는 정도를 나타내는 지표\n",
    "4. 엔트로피: 열역학에서 분자의 무질서함을 측정하는 용어이지만, 정보이론에서 메시지의 평균 정보 양을 측정. 의사결정나무에서는 한 노드의 불순도 측정방법으로 사용\n",
    "5. OneHotEncoding: 범주형 변수를 수치형 변수로 변경하는 방법 중 하나로, 한 특성만 1이고 나머지는 0으로 변경하는 방안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 의사결정나무 ##\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나무 시각화\n",
    "from dtreeplt import dtreeplt\n",
    "dtree = dtreeplt(model=tree, feature_names=x.columns, target_names=['Yes','No'])\n",
    "fig = dtree.view()\n",
    "\n",
    "# 점 시각화\n",
    "x['predict'] = tree.predict(x)\n",
    "x_yes = x[x['predict'] == 'Yes'].copy()\n",
    "x_no = x[x['predict'] == 'No'].copy()\n",
    "plt.plot(x_yes[], x_yes[], 'bo')\n",
    "plt.plot(x_no[], x_no[], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## max_depth수에 따른 AUC 평균값 변화 ##\n",
    "\n",
    "score_list = []\n",
    "I = range(2,11)\n",
    "\n",
    "for i in I:\n",
    "    tree_cv = DecisionTreeClassifier(criterion='entropy', max_depth=i)\n",
    "    scores = cross_val_score(tree_cv, X_ohe, y, scoring='roc_auc', cv=5)\n",
    "    score_list.append(scores.mean())\n",
    "    print(\"AUC score with max_depth {}: {:.3f}\".format(i,scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순수도\n",
    "하나의 노드에 서로 다른 범주의 데이터가 섞여있는 정도를 나타내는 지표\n",
    "- 어떤 규칙으로 나누었을 때 자식 노드의 순수도가 가장 높아지는 지 파악하기 위해 모든 분할 실시\n",
    "- 결측값 존재 = 하나의 범주로 처리\n",
    "- 의사결정나무가 최대로 분할된 경우: 최대 나무"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지니 불순도 : 낮을수록 좋음\n",
    "tree = DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "# 엔트로피 : 낮을수록 좋음\n",
    "tree = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 규제 매개변수들\n",
    "- 과적합을 피하기 위해 규제해야됨\n",
    "- max_depth : 최대깊이 (줄이면 과적합 위험 감소)\n",
    "- min_samples_split : 최소 데이터 수 (해당 노드가 자식 노드로 분할되기 위해 가져야 하는 최소 데이터 수)\n",
    "- min_samples_leaf : 말단 노드 최소 데이터 수 (말단 노드가 가지고 있어야 할 최소 데이터 수)\n",
    "- min_weight_fraction_leaf : 말단 노드 최소 데이터 수 (말단 노드가 가지고 있어야 할 전체 데이터 대비 비율)\n",
    "- max_leaf_nodes : 말단 노드 최대 수 (의사결정나무가 가질 수 있는 말단 노드의 최대 수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력 변수 중요도 및 막대그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"입력 변수 중요도:\\n{}\".format(tree.feature_importances_))\n",
    "\n",
    "def plot_feature_importances(model):\n",
    "    n_features = model.feature_importances_.shape[0]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), x.columns)\n",
    "    plt.xlabel(\"Feature Importances\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.ylim(-1, n_features)\n",
    "    \n",
    "plot_feature_importances(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치를 예측하는 회귀 문제에도 적용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(x,y)\n",
    "\n",
    "# 시각화\n",
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree_reg, out_file='regression_tree.dot',\n",
    "                feature_names=x.columns,\n",
    "                rounded=True,\n",
    "                filled=True)\n",
    "Source.from_file('regression_tree.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 범주형 변수 인코딩\n",
    "수치값을 갖지 않는 범주형 변수들은 수치값을 갖는 형태로 변환하여 기계학습에 활용\n",
    "- 이진형 (Binary): 두 가지 범주 중에서만 값을 가지는 경우 (Yes/No, True/False)\n",
    "\n",
    "=> 변수가 두가지 범주의 값만을 가지므로 하나의 범주는 0으로 나머지 범주는 1로 변환\n",
    "- 서열형 (Ordinal): 범주 값이 순서를 갖는 경우 (성적: A, B, C, D, 경제수준: 상, 중, 하)\n",
    "\n",
    "=> 범주의 순서가 의미가 있기 때문에 순서에 따라서 범주에 정수 혹은 실수의 값 지정\n",
    "\n",
    "- 명목형 (Nominal): 수치적인 의미는 없고 대상을 구분하기 위해 범주를 부여한 경우 (성별: 남, 녀, 혈액형: A, B, O, AB)\n",
    "\n",
    "=> One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진형 (아래 코드 둘 중 아무거나)\n",
    "data['gender'] = data['sex'].replace({'M':0, 'F':1})\n",
    "data['gender'] = (data['sex'] == 'F').astype(np.int)\n",
    "\n",
    "\n",
    "# 서열형 (각각지정 / LabelEncoder / OrdinalEncoder / map)\n",
    "data['Mcode'] = [    'L' if x <= 206468\n",
    "                else 'H' if x>1022440\n",
    "                else 'M' for x in data['Monetary']]\n",
    "data.Mcode.value_counts()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['Mcode_ord'] = label_encoder.fit_transform(data['Mcode'])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "data['Mcode_ord_3'] = ordinal_encoder.fit_transform(data.Mcode.values.reshape(-1,1))\n",
    "\n",
    "ord = {'H':2, 'M':1, 'L':0}\n",
    "data['Mcode_ord_2'] = data.Mcode.map(ord)\n",
    "\n",
    "\n",
    "# 명목형\n",
    "x_ohe = pd.get_dummies(x)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohc = OneHotEncoder()\n",
    "ohe = ohc.fit_transform(data.Mcode.values.reshape(-1,1)).toarray()\n",
    "dfOneHot = pd.DataFrame(ohe, columns=['Mcode_'+str(ohc.categories_[0][i])\n",
    "                                     for i in range(len(ohc.categories_[0]))])\n",
    "data = pd.concat([data, dfOneHot], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6주차 나이브베이스 (분류!)\n",
    "1. 나이브 베이스 알고리즘: 베이스 정리를 활용하여 현재는 알지 못하는 어떤 사건이 일어났을 때 다른 사건이 발생할 확률을 학습하는 방안\n",
    "2. 지지 벡터(Support Vector): 서포트 벡터 머신에서 마진의 경계를 이루는 부분\n",
    "3. 마진 : 서포트 벡터 머신에서 두 가지 범주에 대한 지지 벡터 사이의 넓이이며, 마진이 큰 서포트 벡터 머신 분류 모형을 찾도록 학습함\n",
    "4. 마진 오류 : 완벽하게 분류되는 하드 마진 분류가 아니라, 완벽하게 분류하기 어려운 소프트 마진 분류에서 잘못 분류된 데이터가 어느 정도의 오차를 갖는 지를 나타냄\n",
    "5. 커널 변환 : 선형 분류 모형으로 분류가 어려운 데이터를 변환하여 분류가 손쉽게 이루어지도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나이브 베이즈\n",
    "- GaussianNB: 연속적인 데이터에 적용\n",
    "- BernoulliNB: 이진 데이터에 적용\n",
    "- MultinomialNB: 어떤 것을 헤아린 정수 카운트 데이터에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이진 분류 ##\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "y_pred = gnb.predict(x_test)\n",
    "gnb.score(x_test, y_test)\n",
    "\n",
    "y_prob = gnb.predict_proba(x_test)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신\n",
    "- LinearSVM : 마진을 가장 크게 하는 선형분류 모형을 찾아낼 수 있음\n",
    "- 하드 마진 분류 : 두가지 범주를 완벽하게 분류\n",
    "- 소프트 마진 분류 : 마진을 폭을 가능한 넓게 유지하고 마진 오류 사이에 적절한 균형 잡기\n",
    "- SVM도 스케일에 민감하므로 scaler해주기 -> StandardScaler\n",
    "- SVM 모델이 과대적합이라면 C를 감소시켜 모델 규제 (C가 낮을수록 마진폭 넓음)\n",
    "- LinearSVC가 SVC(kernel='linaer') 보다 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_s = pd.DataFrame(scaler.fit_transform(x))\n",
    "x_ohe = pd.get_dummies(x_s)\n",
    "x_ohe.head()\n",
    "\n",
    "linear_svm = LinearSVC(C=1)\n",
    "linear_svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신 - 다항식 커널\n",
    "- 낮은 차수의 다항식은 매우 복잡한 비선형 데이터 집합을 잘 분류하지 못할 수 있으며 높은 차수의 다항식은 굉장히 많은 특성을 추가하므로 모형을 느리게 만듦\n",
    "- 커널 트릭을 통해 실제로는 특성을 추가하지 않으면서 다항식 특성을 많이 추가한 것과 같은 결과를 얻을 수 있음\n",
    "- kernel='poly' : 다항실 커널 활용\n",
    "- degree=3 : 3차원인 다항식 특성 추가하는 것과 같은 효과\n",
    "- coef0 : 모형이 높은 차수와 낮은 차수에 얼마나 영향을 받을지를 조절\n",
    "- C : 마진 오류를 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "x, y = make_moons(n_samples=100, noise=0.15)\n",
    "svm_clf = SVC(kernel='poly', degree=3, coef0=1, C=5)\n",
    "\n",
    "\n",
    "## 시각화 ##\n",
    "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "## np.ravel() makes multiple dimension array into one dimension\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx,yy,Z,cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(U2[:,0],U2[:,1],c=y)\n",
    "plt.title(\"d = 3, coef0 = 1, C = 5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신 - RBF 커널\n",
    "- Gaussian RBF 커널: 데이터 집합에 임의의 랜드마크를 설정하고 각 데이터가 랜드마크와 얼마나 유사한지를 측정하여 각 데이터의 값을 랜드마크와의 유사도로 변환하는 방안\n",
    "- 랜드마크와 같은 위치일 경우 1, 아주 멀리 떨어진 경우는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_clf = SVC(kernel=\"rbf\", gamma=5, C=0.001) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "rbf_svm_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, coef0=1, tol=0.001, C=5, epsilon=0.1) \n",
    "svm_poly_reg.fit(X_train,y_train)\n",
    "y_pred = svm_poly_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7주차 앙상블과 랜덤포레스트\n",
    "1. 앙상블: 분류 및 예측 알고리즘을 통해 각각 모형을 만든 후에 각 모형으로부터 나온 결과를 취합하여 최종 결과를 내리는 방안\n",
    "2. 배깅과 페이스팅: 학습데이터에서 무작위로 데이터를 선택해서 여러 개의 부분집합을 만들고 이를 같은 알고리즘을 사용하여 여러 개의 모형을 학습하여 결과를 취합하는 방안. 하나의 부분집합에 데이터 중복을 허용하는 게 배깅 방안\n",
    "3. 랜덤 포레스트: 의사결정나무를 배깅하는 방안을 사용해도 되지만 의사결정나무에 맞게 다양한 요소가 고려된 방안\n",
    "4. 부스팅: Boosting은 여러 개의 학습모형을 연결하여 앞의 모형을 보완해 가면서 뒤로 갈수록 더욱 강한 모형을 만드는 앙상블 방안"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 투표 기반 앙상블\n",
    "- 직접 투표 : 여러 모형의 결과를 모아서 다수결로 결정하는 방안\n",
    "- 간접 투표 : 모든 분류기가 클래스의 확률 예측할 수 있으면 개별 분류기의 예측을 평균내어 확률이 가장 높은 클래스를 예측 / 분류 결과를 yes/no, 1/0이 아니라 yes 혹은 no일 확률로 구하여 각 분류모형으로부터 나온 확률값의 평균을 취해 최종 결정을 하는 방안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 직접 투표 ##\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "log_clf = LogisticRegression(max_iter=100000)\n",
    "svm_clf = SVC()\n",
    "nb_clf = GaussianNB()\n",
    "voting_clf = VotingClassifier(estimators = [('log', log_clf), \n",
    "                                            ('svm', svm_clf),\n",
    "                                            ('nb', nb_clf)], voting='hard')\n",
    "voting_clf.fit(x_train, y_train)\n",
    "for clf in (log_clf, svm_clf, nb_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "## 간접 투표 ##\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "log_clf = LogisticRegression(max_iter=100000)\n",
    "svm_clf = SVC(probability=True)\n",
    "nb_clf = GaussianNB()\n",
    "voting_clf = VotingClassifier(estimators = [('log', log_clf), \n",
    "                                            ('svm', svm_clf),\n",
    "                                            ('nb', nb_clf)], voting='soft')\n",
    "voting_clf.fit(x_train, y_train)\n",
    "for clf in (log_clf, svm_clf, nb_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배깅과 페이스팅\n",
    "- 배깅: 한 집합에 데이터 중복 허용 => bootstrap=True\n",
    "- 페이스팅: 한 집합에 데이터 중복 불허용 => bootstrap=False\n",
    "- 분류 문제에서는 하드 보팅 방식처럼 다수결의 원칙, 수치를 예측하는 회귀문제는 분류 모형 예측치의 평균값 사용\n",
    "- 부분집합데이터가 편향된 데이터일 수 있지만 앙상블을 통해 편향과 분산을 감소시킬 수 있음\n",
    "- n_jobs : 훈련과 에측에 사용할 CPU 코어 수 지정 (n_jobs=-1 : 가용한 모든 CPU 코어 사용)\n",
    "- max_features : 각 모형에 활용되는 변수의 최대 비율\n",
    "- bootstrap_features=True : 같은 변수를 중복해서 입력\n",
    "- 랜덤패치 : 데이터와 변수를 모두 무작위로 선택하여 학습\n",
    "- 랜덤 서브스페이스 : 모형이 데이터는 전부사용하지만 변수만 무작위로 선택하여 학습bootstrap=False, max_samples=1.0, bootstrap_features=True, max_features<1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                            n_estimators=500,\n",
    "                            max_samples=100,\n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1)\n",
    "bag_clf.fit(x_train, y_train)\n",
    "y_pred = bag_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트\n",
    "RandomForestClassifier() == BaggingClassifier(DecisionTreeClassifier)\n",
    "- max_features = 'auto' : 전체 변수 수에 root를 씌운 값 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500,\n",
    "                                 max_leaf_nodes = 16,\n",
    "                                 max_features = 'auto',\n",
    "                                 max_samples=0.5,\n",
    "                                 bootstrap = True,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_leaf_nodes = 16),\n",
    "                            n_estimators = 500,\n",
    "                            max_features = 'auto',\n",
    "                            max_samples=0.5,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "# 변수의 중요도 산출\n",
    "for name, score in zip(x.columns, rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엑스트라 트리\n",
    "- 무작위로 선택한 변수 중에서 무작위로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier(n_estimators=500,\n",
    "                              max_leaf_nodes=16,\n",
    "                              max_features='auto',\n",
    "                              n_jobs=-1)\n",
    "et_clf.fit(x_train, y_train)\n",
    "y_pred = et_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치 예측 문제에 적용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "tree_rnd = RandomForestRegressor(n_estimators=500,\n",
    "                                 max_leaf_nodes=16,\n",
    "                                 max_features='auto',\n",
    "                                 max_samples=0.5,\n",
    "                                 bootstrap=True,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "tree_et = ExtraTreesRegressor(n_estimators=500,\n",
    "                              max_leaf_nodes=16,\n",
    "                              max_features='auto',\n",
    "                              max_samples=0.5,\n",
    "                              bootstrap=True,\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부스팅\n",
    "- 여러개의 학습모형을 연결하여 앞의 모형을 보완해 가면서 뒤로 갈수록 더욱 강한 모형 만들기\n",
    "- 에이다 부스트 : 이전 모형이 잘 맞추진 못한 데이터에 대해서 가중치 높이기 (학습률 값이 크면 더 많은 가중치 변화를 가져옴)\n",
    "- 그래디언트 부스트 : 반복마다 데이터의 가중치를 수정하는 대신 이전 모형이 만든 잔여 오차에 새로운 모형을 학습시킴 (분류/수치예측 모두 적용 가능)\n",
    "- 확률적 그래디언트 부스트 : 학습 데이터의 비율 지정\n",
    "- 익스트림 그래디언트 부스트 : 젤 조음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 에이다 부스트 ##\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                             n_estimators=200,\n",
    "                             algorithm='SAMME.R',\n",
    "                             learning_rate=0.5)\n",
    "\n",
    "\n",
    "\n",
    "## 그래디언트 부스트 ##\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(x,y)\n",
    "\n",
    "errors = [math.sqrt(mean_squared_error(y_val, y_pred)) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "print(bst_n_estimators)\n",
    "\n",
    "\n",
    "\n",
    "## 익스트림 그래디언트 부스트 ##\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor(max_depth=2, eta=1)\n",
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
